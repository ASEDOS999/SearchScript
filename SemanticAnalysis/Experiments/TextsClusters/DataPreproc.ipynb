{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Separate All Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ased/anaconda3/lib/python3.5/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text3_11.txt\n",
      "text1_1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ased/Desktop/PROJECTS/SearchScript/SemanticAnalysis/Experiments/TextsClusters/clusterization.py:91: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return s/k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text2_17.txt\n",
      "text3_12.txt\n",
      "text3_5.txt\n",
      "text2_2.txt\n",
      "text2_3.txt\n",
      "text1_6.txt\n",
      "text1_5.txt\n",
      "text2_14.txt\n",
      "text2_22.txt\n",
      "text2_18.txt\n",
      "text2_16.txt\n",
      "text3_6.txt\n",
      "text3_9.txt\n",
      "text2_13.txt\n",
      "text3_2.txt\n",
      "text3_4.txt\n",
      "text2_6.txt\n",
      "text3_17.txt\n",
      "text3_16.txt\n",
      "text2_5.txt\n",
      "text2_20.txt\n",
      "text3_15.txt\n",
      "text1_3.txt\n",
      "text2_4.txt\n",
      "text2_11.txt\n",
      "text2_8.txt\n",
      "text1_2.txt\n",
      "text1_4.txt\n",
      "text3_1.txt\n",
      "text3_8.txt\n",
      "text3_14.txt\n",
      "text2_9.txt\n",
      "text2_21.txt\n",
      "text2_1.txt\n",
      "text2_7.txt\n",
      "text2_12.txt\n",
      "text3_13.txt\n",
      "text2_10.txt\n",
      "text3_7.txt\n",
      "text3_10.txt\n",
      "text3_3.txt\n",
      "text2_19.txt\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import clusterization\n",
    "import gensim\n",
    "\n",
    "with open('../../table.pickle', 'rb') as f:\n",
    "    table = pickle.load(f)\n",
    "    f.close()\n",
    "with open('NotMarkedSegments.pickle', 'wb') as f:\n",
    "    pickle.dump(dict(),f)\n",
    "    f.close()\n",
    "list_files = list(table.keys())\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('../../model.bin', binary=True) \n",
    "model.init_sims(replace=True)\n",
    "for i in list_files:\n",
    "    print(i)\n",
    "    list_texts, TT, TagUd = clusterization.trivial_segmentation(i, model, table[i.split('/')[-1]])\n",
    "    list_texts, TT, TagUd = clusterization.union(list_texts, TT, TagUd, model)\n",
    "    with open('SegmentTexts/'+i.split('/')[-1].split('.')[0]+'.md','w') as f:\n",
    "        for j in list_texts:\n",
    "            f.write('**NEW SEGMENT**\\n\\n'+j+'\\n\\n')\n",
    "        f.close()\n",
    "    with open('NotMarkedSegments.pickle', 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "        f.close()\n",
    "    with open('NotMarkedSegments.pickle', 'wb') as f:\n",
    "        d[i.split('/')[-1].split('.')[0]] = [(i, TT[ind], TagUd[ind]) for ind, i in enumerate(list_texts)]\n",
    "        pickle.dump(d,f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from preprocessing import full_extracting\n",
    "marks = full_extracting('NewMarkedTexts/')\n",
    "with open('NotMarkedSegments.pickle', 'rb') as f:\n",
    "    segments = pickle.load(f)\n",
    "    f.close()\n",
    "marked_segments = dict()\n",
    "for key in segments:\n",
    "    if marks.__contains__(key):\n",
    "        marked_segments[key] = [(elem[0], elem[1], elem[2], marks[key][ind])\n",
    "                             for ind, elem in enumerate(segments[key])]\n",
    "    else:\n",
    "        marked_segments[key] = [(elem[0], elem[1], elem[2], None)\n",
    "                             for ind, elem in enumerate(segments[key])]\n",
    "with open('MarkedSegments.pickle', 'wb') as f:\n",
    "    pickle.dump(marked_segments, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
