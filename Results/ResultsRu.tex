 \documentclass[12pt]{article}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}       

\usepackage[english]{babel}
\usepackage{amsmath,amsfonts,amsthm,amssymb,amsbsy,amstext,amscd,amsxtra,multicol}
\usepackage{verbatim}
\usepackage{tikz}
\usetikzlibrary{automata,positioning}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage[colorlinks,urlcolor=blue]{hyperref}
\usepackage[stable]{footmisc}
\usepackage{ dsfont }
\usepackage{wrapfig}
\usepackage{xparse}
\usepackage{ifthen}
\usepackage{bm}
\usepackage{color}

\usepackage{algorithm}
\usepackage{algpseudocode}
	
\usepackage{xcolor}
\usepackage{hyperref}
\definecolor{linkcolor}{HTML}{799B03} % цвет гиперссылок
\definecolor{urlcolor}{HTML}{799B03} % цвет гиперссылок
 
%\hypersetup{pdfstartview=FitH,  linkcolor=linkcolor,urlcolor=urlcolor, colorlinks=true}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\grad}{grad}
\DeclareMathOperator{\intt}{int}
\DeclareMathOperator{\conv}{conv}
\begin{document}

\tableofcontents
\newpage

\section{Описание метода}

На входе мы имеем несколько тесктов, в которых содержатся рекомендации, советы и т.д. по поводу достижения некоторой единой для всех текстов цели. На выходе мы хотим получить на основе этих текстов общую схему действий для достижения этой цели.

На данный момент мы предполагаем получение, что все советы прописаны достаточно явно. Мы не предполагаем, что нам нужно читать какой-либо личный рассказ о том, как человек достигал этой цели, и в соответствии с анализом этого текста вносить изменения в схему.

\subsection{Основные этапы выделения сценария}
\label{marker1}
Обозначим основные этапы выделения сценария:

1. Выделение из каждого текста информации, возможно относящейся к сценарию.

2. Струтуризация информации для каждого текста согласно единому шаблону.

3. Выбор предложений, удовлетворяющих необходимым условиям для вхождения в сценарий.

4. Формирование окончательного сценария на основе полученных на предыдущем шаге выборок.

Теперь разберем каждый из этих этапов. Анализ, насколько идея и реализация успешны, будет в сецкии \nameref{marker7}.

\subsection{Выделение Информации Из Исходных Текстов}
\label{marker2}

Рассмотрим отдельный текст со входа. Мы предполагаем, что полезную информацию могут содержать только те элементы текста, которые относятся к одному из двух типов:

\begin{itemize}
	\item Предложения, содержащие глаголы и отглагольные части речи,
	\item Списки внутри текста.
\end{itemize} 

Разберем, почему выделили именно эти два типа.

Схема, которую мы хотим получить в конце и которую мы называем сценарием, по сути есть компиляция из всех советов, рекомендаций и указаний, находящихся в текстах. Эти элементы есть побуждения к действию, а действие всегда за редким исключением выражаются через глагол и отглагольные части речи. По сути это объясняет и почему мы пренебрегаем остальными предложениями.

Однако есть такие элементы, как списки. В некотором смысле, список достаточно часто есть продолжение предыдущих предложений и содержит особо важную выделенную информацию. Это может быть список того, что нужно взять, или того, какие вопросы следует задать. И потому нам не следует ими пренебрегать.

\textbf{О реализации.} Нахождение предложений с глаголами сводится к задаче сегментации текста на предложение и опреднию того, является ли слово глаголом. Для этого существует достаточно большое разнообразие парсеров, в нашем случае мы использовали isanlp. Далее любой найденный глагол или отглагольная часть речи вместе со своими зависимостями будет называться действием.

Теперь поговорим о списках. В нашем представлении список выглядит следующем образом: это череда абзацев, которые начинаются с определенных символов - для нумерованного списка это числа или буквы, для ненумерованного - различные символы, например, '*', '-', '+' и т.д. Так же мы учитываем возможность, что после каждого маркированного абзаца, являющегося элементом списка, идет еще один абзац, который является разъясняющим к первому. Использование этих критериев позволяет легко выделять списки.

Здесь стоит заметить, что список в тексте это необязательно несколько однотипно начинающихся обзацев и он может быть более сложно устроен. В частности, каждый элемент списка может быть представлен более, чем двумя абзацами, или список может содержать вложенные списки. Однако мы пренебрегаем данным фактом, поскольку мы предполагаем, если такие сложные структуры имеют место быть, то каждый элемент списка и так должен содержать некоторые дополнительную информацию, которая будет учтена, как предложения с глаголами. И в результате мы не проиграем сильно от того, что не знаем, что это список.

% Здесь еще нужно поговорить о нахождении главного слова. Но перед этим желательно это реализовать :))


\subsection{Структуризация Информации}
\label{marker3}

Данный этап является надстройкой над предыдущим. В результате предыдущего этапа мы научились выделять действия и списки. В результате этого этапа мы получаем следующую структуру данных: каждый элемент это либо абзац, либо список. Элементы упорядочены согласно встречанию в тексте. Каждый элемент-абзац содержит все действия, содержащиеся в нем, и название секции, к которой он принаджежит. Каждый элемент-список содержит составляющие его элементы-абзацы и название секции, к которой он принадлежит.

Немного обсудим название секций. Большинство текстов содержат некоторую внутренную структуру, которая выражается в первую очередь за счет заголовков разных уровней (заголовки, подзаголовки, подзаголовки для подзаголовков и т.д.). И логично предположить, что эти заголовки смогут помочь при дальнейшем объединении извлеченной из разных текстов информации в единый сценарий. Для элемента-абзаца, который является частью списка, мы считаем названием секции первое предложение соответствующего элемента списка. Для элементов-списков и всех остальных элементов-абзацев мы считаем названием секции заголовок самого нижнего уровня.

Выделение заголовков построено на крайне простой идеи: каждый заголовок представляет собой абзац, состоящий из единственного предложения. Причем это предложение удовлетворяет двум условиям: оно не очень длинное (эмперически подобранно ограничение не более 10 слов) и оно либо заканчивается обычными для окончания предложения знаками припинания (тока, вопросительный или восклицательный знак), либо в конце не стоит никакого знака припинания.

\subsection{Необходимые Условия Вхождения В Сценарий}
\label{marker4}

В результате предыдущих этапов мы получили все действия, которые есть в тексте, и структуризировали их. На этом этапе мы определимся как, используя морфологическую и синтаксическую информацию, оставить то, что нам с крайне высокой вероятностью подходит.

За время анализа текстов были выделены следующие условия:

\begin{itemize}
	\item Глагол в повелительной форме
	\item Глагол в форме инфинитива, причем этот глагол зависит от таких слов, как 'можно', 'нужно' и т.д.
	\item Глаголы во втором лице
\end{itemize}

Все эти условия так же проверяются, используя инструменты из библиотеки isanlp.

В результате отработки всех предыдущих и этого этапов для каждого текста мы имеем экстракт, который отчасти уже является отдельным сценарием для этого текста. Он имеет два важных следующих недостатка:
\begin{itemize}
	\item содержит некоторые действия, которые не несут никакой смысловой информации
	\item не рассматривает возможность ветвления сценария
\end{itemize}

Устранение первого недостатка, предположительно, произойдет при сравнении сценариев, извлеченных из разных текстов. В частности этому посвящены следующие этапы.

\subsection{Формирование Единого Сценария}
\label{marker5}

Для всех дальнейших экспериментов, связанных с семантическим анализом были использованы модели из RusVectores и модуль gensim.

\subsubsection{Семантический Анализ}

Цель данного этапа: для любой пары действий определить близки ли эти действия.

%нужно проделать эксперименты и дописать

\subsubsection{Объединение Выборок}
\label{marker6}

В результате предыдущего этапа для любой пары действий мы знаем, близки ли они по смыслу или нет.

...

% Дописать, что мы с этой несчастной близостью делаем
\section{Результаты И Анализ}
\label{marker7}

\subsection{Данные}
Мы работали с выборкой текстов из интернета, которые есть инструкции по покупке автомобиля. Всего в выборке находится 35 документов. Вся выборка состоит из трех частей: инструкции для подержанных автомобилей, инструкции для новых и остальные тексты. Каждая из этих выборок содержит 13, 17 и 5 документов соответственно.

\subsection{До семантического анализа}

В данном разделе, мы обсудим результаты работы первых трех этапов. Заметим, что эти результаты во многом зависят от качества используемого парсера.

О выделении списков. Среди всех 35 документов не было обнаружено такого, что текст в нем содержит список и он не был бы обнаружен. Поэтому критерии выделения списка, описанные в \nameref{marker2}, можно считать сформулированными корректно.

О качестве получаемой выборки из текста. Для начала, давайте определимся, как исследовать это качество. Результат первых трех этапов есть выборка, состоящая из действий и списков. Давайте считать каждый элемент списка отдельной единицей информации. Так же пусть у нас имеется выделенный в ручную сценарий. Нас будут интересовать две величины:
\begin{itemize}
	\item сколько из вручную выделенного сценария элементов было выделено алгоритмически,
	\item сколько из выделеных алгоритмически элементов входит во множество вручную выделенных элементов.
\end{itemize}

Т.е. какую часть истинного сценария мы смогли найти и какая часть выборки является нужной. Легко заметить, что это хорошо известные метрики Recall и Precision. Было выбрано два текста и результаты вы можете найти в таблице~\ref{table:1}.

%Results from Demonstration/Prog_VS_Hand/Demonstration.ipynb
\begin{table}[h!]
\centering
\begin{tabular}{||c |c |c||} 
 \hline
 Col1 & Recall,$\%$ & Precision, $\%$\\
 \hline
 Text 1 &  97.4&  97.6\\ 
 Text 2 &  83.7&  87.8\\
 \hline
 Mean & 90.6& 92.7\\
 \hline
\end{tabular}
\caption{Качество выбираемой информации}
\label{table:1}
\end{table}

Как можно видеть, наш метод показывает достаточно хорошие результаты. При непосредственном ознакомлении с текстами, можно заметить, что больше всего выделяется ненужной информации в начале (предисловии, которое достаточно часто содержится в текстах и не несет сильной смысловой нагрузки) и в конце (пожелания, напутственные слова и т.д.).

\subsection{Семантический анализ}

\subsubsection{Семантический Анализ: Предложения}

После предыдущего этапа для каждого текста мы имеем предложения, в которых содержатся действия. На данном этапе создадим матрицу расстояний $D$ для всех предложений из всех текстов. Т.е. это матрица, в которой $D_{ij}$ - это расстояние между $i$-ым и $j$-ым предложением. Для измерения попарного расстояния между предложениями был выбраны модели \textbf{word2vec} и метод \textbf{WMD (Word Mover's Distance)}.

Увидеть распределение пар предложений по расстоянию вы можете на рис.~\ref{fig:image1}. Назовем два действия $\epsilon$-\textbf{близкими по предложению}, если расстояние между предложениями, в которых эти действия содержатся, меньше $\epsilon$.

\subsubsection{Семантический Анализ: Сравнение Действий}

Цель данного этапа: найти расстояния между двумя действиями. В качестве основных идей предлагаются следующие:
\begin{itemize}
	\item Использование расстояний между предложениями и некоторого порогового значения расстояния
	\item Использовать одну из моделей ML и обучить её:
	\begin{itemize}
		\item Линейные модели
		\item Random Forest
		\item Нейросетевые протоколы
	\end{itemize}
\end{itemize}

\begin{figure}[h!]
\center{\includegraphics[scale=1.]{Images/distance_distribution.pdf}}
\label{fig:image1}
 \caption{Распределение пар предложений}
\end{figure}


В результате экспериментов было решено остановиться на ... %нужно проделать эксперименты и дописать


\subsection{Окончательный результат}

\end{document}
